{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasets/q3/train.csv', sep=',')\n",
    "test_data = pd.read_csv('datasets/q3/test_without_labels.csv', sep=',')\n",
    "\n",
    "more_stop_words = ['made','whoever','who','said','day','will','new','now','time','say','second','month','first','going','year','back','people','case','according']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17abcdb75c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Label',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = train_data['Content'].head(10000)\n",
    "#y = train_data['Label'].head(10000)\n",
    "X = train_data['Content']\n",
    "y = train_data['Label']\n",
    "X = np.array(X).flatten()\n",
    "y = np.array(y).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "\n",
    "stopWords = []\n",
    "stopWords.extend(['saying', 'said', 'say', 'yes', 'instead', 'meanwhile', 'now', 'one', 'suggested', 'says', 'added','know', 'though', 'let', 'going', 'back',\n",
    "                       'well', 'example', 'us', 'yet', 'perhaps', 'actually', 'year', 'lastyear','maybe', 'ask', '.', ',', ':', 'take', 'n\\'t', 'go', \n",
    "                       'make', 'two', 'got', 'took','get', '?', 'would', '(', '\\'', ')', '``', '/', \"''\", '%', '#', '!', \"'s\", ';', '[', ']', '...',\n",
    "                       \"'m\", \"'d\", 'also', 'something', 'even', 'new', 'lot', 'a', 'thing', 'time', 'way','whose', 'people', 'come'])\n",
    "\n",
    "stopWords.extend([' ','are','and','I','A','And','So','This','When','It','so','These','these'])\n",
    "stopWords.extend(more_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['accord', 'actual', 'ad', 'd', 'exampl', 'i', 'it', 'm', 'mayb', 'meanwhil', 'nt', 'peopl', 'perhap', 's', 'someth', 'suggest', 'this', 'when'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.89184\n",
      "Precision =  0.8920063357556798\n",
      "Recall =  0.891880753937008\n",
      "f1-score =  0.8918126858756633\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#stopWords = stopwords.words('english')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def processText(text):    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\" can't \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cannot \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" isn't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ain't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint' \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aren't \", \" are not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" mustn't \", \" must not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" shouldn't \", \" should not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" terrible \", \" awful \", text, flags=re.IGNORECASE)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    #stems = ' '.join(stems)\n",
    "    #stems = ngrams(stems,3)\n",
    "    return stems\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=processText, stop_words = stopWords,use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(tokenizer=processText, stop_words = 'english',max_df=0.9, min_df=3)\n",
    "\n",
    "\n",
    "nb = SVC(kernel='rbf',C=8,gamma=0.1)    \n",
    "#nb = KNeighborsClassifier(n_neighbors=3)\n",
    "#nb = ComplementNB()\n",
    "\n",
    "estimators = [(\"tf_idf\",vectorizer),(\"svm\",nb)]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model.fit(X[trainI],y[trainI])\n",
    "    predictions = model.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def processText2(text):    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\" can't \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cannot \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" isn't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ain't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint' \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aren't \", \" are not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" mustn't \", \" must not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" shouldn't \", \" should not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" terrible \", \" awful \", text, flags=re.IGNORECASE)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    stems = [wrd for wrd in stems if(wrd not in stopWords)]\n",
    "    stems = ' '.join(stems)\n",
    "    return stems\n",
    "\n",
    "temp_train = train_data['Content']\n",
    "#for i in temp_train:\n",
    "#    i = processText2(i)\n",
    "\n",
    "X_train = temp_train.map(processText2)\n",
    "y = train_data['Label']\n",
    "    \n",
    "#X = np.array(temp_train).flatten()\n",
    "#y = np.array(train_data['Label'].head(10000)).flatten()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_train_seq = pd.Series(tokenizer.texts_to_sequences(X))\n",
    "X_train_pad = pad_sequences(X_train_seq)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 380s - loss: 0.4701 - accuracy: 0.7714\n",
      "Epoch 2/2\n",
      " - 381s - loss: 0.2467 - accuracy: 0.9105\n",
      "Epoch 1/2\n",
      " - 380s - loss: 0.2046 - accuracy: 0.9278\n",
      "Epoch 2/2\n",
      " - 385s - loss: 0.1416 - accuracy: 0.9528\n",
      "Epoch 1/2\n",
      " - 381s - loss: 0.1295 - accuracy: 0.9588\n",
      "Epoch 2/2\n",
      " - 381s - loss: 0.0834 - accuracy: 0.9765\n",
      "Epoch 1/2\n",
      " - 381s - loss: 0.0836 - accuracy: 0.9754\n",
      "Epoch 2/2\n",
      " - 381s - loss: 0.0524 - accuracy: 0.9853\n",
      "Epoch 1/2\n",
      " - 381s - loss: 0.0523 - accuracy: 0.9851\n",
      "Epoch 2/2\n",
      " - 382s - loss: 0.0336 - accuracy: 0.9913\n",
      "Accuracy =  0.9437199999999999\n",
      "Precision =  0.9444293608727607\n",
      "Recall =  0.9436854828686458\n",
      "f1-score =  0.943655735634399\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "#X = np.array(X_train_pad).flatten()\n",
    "#y = np.array(y).flatten\n",
    "\n",
    "#model.fit(x=X_train_pad, y=y, batch_size=batch_size, epochs=epochs, validation_split=0.01)\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model.fit(X_train_pad[trainI], y[trainI], batch_size=batch_size, epochs=epochs,verbose=2)\n",
    "    predictions = model.predict_classes(X_train_pad[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data['Content']\n",
    "test_data.map(processText2)\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=6000)\n",
    "tokenizer.fit_on_texts(test_data)\n",
    "X_test_seq = pd.Series(tokenizer.texts_to_sequences(test_data))\n",
    "X_test_pad = pad_sequences(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = model.predict_classes(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('datasets/q3/test_without_labels.csv', sep=',')\n",
    "to_print_arr = np.array(test_data[['Id']]).flatten()\n",
    "columns = pd.Index(['Id','Predicted'])\n",
    "data = np.column_stack((to_print_arr,pred_final))\n",
    "df_final = pd.DataFrame(data, index=None, columns = columns)\n",
    "df_final.to_csv('q3/sentiment_predictions.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Requirement3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
