{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('datasets/q3/train.csv', sep=',')\n",
    "test_data = pd.read_csv('datasets/q3/test_without_labels.csv', sep=',')\n",
    "\n",
    "more_stop_words = ['made','whoever','who','said','day','will','new','now','time','say','second','month','first','going','year','back','people','case','according']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17abcdb75c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Label',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = train_data['Content'].head(10000)\n",
    "#y = train_data['Label'].head(10000)\n",
    "X = train_data['Content']\n",
    "y = train_data['Label']\n",
    "X = np.array(X).flatten()\n",
    "y = np.array(y).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "\n",
    "stopWords = []\n",
    "stopWords.extend(['saying', 'said', 'say', 'yes', 'instead', 'meanwhile', 'now', 'one', 'suggested', 'says', 'added','know', 'though', 'let', 'going', 'back',\n",
    "                       'well', 'example', 'us', 'yet', 'perhaps', 'actually', 'year', 'lastyear','maybe', 'ask', '.', ',', ':', 'take', 'n\\'t', 'go', \n",
    "                       'make', 'two', 'got', 'took','get', '?', 'would', '(', '\\'', ')', '``', '/', \"''\", '%', '#', '!', \"'s\", ';', '[', ']', '...',\n",
    "                       \"'m\", \"'d\", 'also', 'something', 'even', 'new', 'lot', 'a', 'thing', 'time', 'way','whose', 'people', 'come'])\n",
    "\n",
    "stopWords.extend([' ','are','and','I','A','And','So','This','When','It','so','These','these'])\n",
    "stopWords.extend(more_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['accord', 'actual', 'ad', 'd', 'exampl', 'i', 'it', 'm', 'mayb', 'meanwhil', 'nt', 'peopl', 'perhap', 's', 'someth', 'suggest', 'this', 'when'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.89184\n",
      "Precision =  0.8920063357556798\n",
      "Recall =  0.891880753937008\n",
      "f1-score =  0.8918126858756633\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#stopWords = stopwords.words('english')\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def processText(text):    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\" can't \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cannot \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" isn't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ain't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint' \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aren't \", \" are not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" mustn't \", \" must not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" shouldn't \", \" should not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" terrible \", \" awful \", text, flags=re.IGNORECASE)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    #tokens = [wrd for wrd in tokens if(wrd not in stopWords)]\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    #stems = ' '.join(stems)\n",
    "    #stems = ngrams(stems,3)\n",
    "    return stems\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=processText, stop_words = stopWords,use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(tokenizer=processText, stop_words = 'english',max_df=0.9, min_df=3)\n",
    "\n",
    "\n",
    "nb = SVC(kernel='rbf',C=8,gamma=0.1)    \n",
    "#nb = KNeighborsClassifier(n_neighbors=3)\n",
    "#nb = ComplementNB()\n",
    "\n",
    "estimators = [(\"tf_idf\",vectorizer),(\"svm\",nb)]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model.fit(X[trainI],y[trainI])\n",
    "    predictions = model.predict(X[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def processText2(text):    \n",
    "    p_stemmer = SnowballStemmer(language='english')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r\" can't \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cannot \", \" can not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" isn't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ain't \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aint' \", \" is not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" aren't \", \" are not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" mustn't \", \" must not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" shouldn't \", \" should not \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" terrible \", \" awful \", text, flags=re.IGNORECASE)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    stems = [p_stemmer.stem(t) for t in tokens]\n",
    "    stems = [wrd for wrd in stems if(wrd not in stopWords)]\n",
    "    stems = ' '.join(stems)\n",
    "    return stems\n",
    "\n",
    "temp_train = train_data['Content']\n",
    "#for i in temp_train:\n",
    "#    i = processText2(i)\n",
    "\n",
    "X_train = temp_train.map(processText2)\n",
    "y = train_data['Label']\n",
    "    \n",
    "#X = np.array(temp_train).flatten()\n",
    "#y = np.array(train_data['Label'].head(10000)).flatten()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_train_seq = pd.Series(tokenizer.texts_to_sequences(X))\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=512)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim = 32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 86s - loss: 0.4356 - accuracy: 0.7861\n",
      "Epoch 2/2\n",
      " - 86s - loss: 0.2526 - accuracy: 0.9007\n",
      "Epoch 1/2\n",
      " - 87s - loss: 0.2204 - accuracy: 0.9156\n",
      "Epoch 2/2\n",
      " - 90s - loss: 0.1627 - accuracy: 0.9421\n",
      "Epoch 1/2\n",
      " - 87s - loss: 0.1534 - accuracy: 0.9448\n",
      "Epoch 2/2\n",
      " - 89s - loss: 0.1087 - accuracy: 0.9645\n",
      "Epoch 1/2\n",
      " - 89s - loss: 0.1084 - accuracy: 0.9653\n",
      "Epoch 2/2\n",
      " - 95s - loss: 0.0725 - accuracy: 0.9784\n",
      "Epoch 1/2\n",
      " - 91s - loss: 0.0741 - accuracy: 0.9765\n",
      "Epoch 2/2\n",
      " - 87s - loss: 0.0434 - accuracy: 0.9879\n",
      "Accuracy =  0.93604\n",
      "Precision =  0.9364182433508841\n",
      "Recall =  0.9361011283115733\n",
      "f1-score =  0.9360289586417545\n"
     ]
    }
   ],
   "source": [
    "accuracy,precision,recall,f1 = 0,0,0,0\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "#X = np.array(X_train_pad).flatten()\n",
    "#y = np.array(y).flatten\n",
    "\n",
    "#model.fit(x=X_train_pad, y=y, batch_size=batch_size, epochs=epochs, validation_split=0.01)\n",
    "\n",
    "for trainI,testI in kf.split(X):\n",
    "    #kfTestCategIds = le.transform(y[testI])\n",
    "    model.fit(X_train_pad[trainI], y[trainI], batch_size=batch_size, epochs=epochs,verbose=2)\n",
    "    predictions = model.predict_classes(X_train_pad[testI])\n",
    "    \n",
    "    ret = precision_score(y[testI], predictions, average='macro')\n",
    "    precision += ret\n",
    "\n",
    "    ret = recall_score(y[testI], predictions, average='macro')\n",
    "    recall += ret\n",
    "\n",
    "    ret = f1_score(y[testI], predictions, average='macro')\n",
    "    f1 += ret\n",
    "\n",
    "    accuracy += accuracy_score(y[testI], predictions)\n",
    "    \n",
    "#accuracy,precision,recall,f1 = accuracy/float(5),precision/float(5),recall/float(5),f1/float(5)\n",
    "accuracy,precision,recall,f1 = accuracy/5,precision/5,recall/5,f1/5\n",
    "print(\"Accuracy = \",end=' ')\n",
    "print(accuracy)\n",
    "print(\"Precision = \",end=' ')\n",
    "print(precision)\n",
    "print(\"Recall = \",end=' ')\n",
    "print(recall)\n",
    "print(\"f1-score = \",end=' ')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        this has to be of the best movi we have seen w...\n",
       "1        over the last few i have seen of review for th...\n",
       "2        this movi was aw i dont where to beginth onli ...\n",
       "3        the worst wrestlemania everbr br this had no m...\n",
       "4        i saw this film in the theater in the 40s when...\n",
       "                               ...                        \n",
       "24995    this is pretti run of the mill famili move tha...\n",
       "24996    this movi was dumb slow was it ever slow the o...\n",
       "24997    there goodbad movi badbadmovi enjoy bad movies...\n",
       "24998    john huston finish his remark career with of t...\n",
       "24999    im an anim myself an all around buff of the me...\n",
       "Name: Content, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data['Content']\n",
    "test_data.map(processText2)\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.fit_on_texts(test_data)\n",
    "X_test_seq = pd.Series(tokenizer.texts_to_sequences(test_data))\n",
    "X_test_pad = pad_sequences(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = model.predict_classes(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('datasets/q3/test_without_labels.csv', sep=',')\n",
    "to_print_arr = np.array(test_data[['Id']]).flatten()\n",
    "columns = pd.Index(['Id','Predicted'])\n",
    "data = np.column_stack((to_print_arr,pred_final))\n",
    "df_final = pd.DataFrame(data, index=None, columns = columns)\n",
    "df_final.to_csv('q3/sentiment_predictions.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Requirement3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
